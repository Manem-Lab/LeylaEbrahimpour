{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b32deeb4-b740-4e14-8fbc-56edd3f718b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 1111)\n",
      "(208, 1111)\n",
      "(254, 1111)\n",
      "(208, 1111)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from skrebate import ReliefF\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from skrebate import ReliefF, SURF, MultiSURF\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from skrebate import SURF\n",
    "from skrebate import SURFstar\n",
    "from skrebate import MultiSURF\n",
    "from skrebate import MultiSURFstar\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN, KMeansSMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "#file_name = 'cancerprediction_rads_AddDelta_lasso_NoSmote'\n",
    "#Load the data\n",
    "main_dir= \"/home/ulaval.ca/lesee/projects/Project-NLST/\"\n",
    "\n",
    "#data_train_T0 = pd.read_excel(os.path.join(main_dir,'data/radiomicsfeatures_kheops-NLST-Dmitrii-Cohort1_Laptop_v2-cleaned-T0-ML.xlsx'))\n",
    "#data_train_T1 = pd.read_excel(os.path.join(main_dir,'data/radiomicsfeatures_kheops-NLST-Dmitrii-Cohort1_Laptop_v2-cleaned-T1-ML.xlsx'))\n",
    "#data_test_T0 = pd.read_excel(os.path.join(main_dir,'data/radiomicsfeatures_kheops-NLST-Dmitrii-Cohort2_Laptop_v2-cleaned-T0-ML.xlsx'))\n",
    "#data_test_T1 = pd.read_excel(os.path.join(main_dir,'data/radiomicsfeatures_kheops-NLST-Dmitrii-Cohort2_Laptop_v2-cleaned-T1-ML.xlsx'))\n",
    "\n",
    "df_rad_features_T0_train = pd.read_csv(os.path.join(main_dir,'results/radiomics-ComBat_harmonized/cohort1_T0_rads_combat_man_notdropped.csv'))\n",
    "\n",
    "df_rad_features_T0_test = pd.read_csv(os.path.join(main_dir,'results/radiomics-ComBat_harmonized/cohort2_T0_rads_combat_man_notdropped.csv'))\n",
    "df_rad_features_T1_train = pd.read_csv(os.path.join(main_dir,'results/radiomics-ComBat_harmonized/cohort1_T1_rads_combat_man_notdropped.csv'))\n",
    "df_rad_features_T1_test = pd.read_csv(os.path.join(main_dir,'results/radiomics-ComBat_harmonized/cohort2_T1_rads_combat_man_notdropped.csv'))\n",
    "\n",
    "df_labels_train = pd.read_csv(os.path.join(main_dir,'data/data_batch-clinical-labels/Cohort1_labels.csv'))\n",
    "df_labels_test = pd.read_csv(os.path.join(main_dir,'data/data_batch-clinical-labels/Cohort2_labels.csv'))\n",
    "\n",
    "\n",
    "# perform an inner join on the 'PatientID' column\n",
    "data_train_T0 = pd.merge(df_rad_features_T0_train, df_labels_train, on='PatientID', how='inner')\n",
    "data_test_T0 = pd.merge(df_rad_features_T0_test, df_labels_test, on='PatientID', how='inner')\n",
    "data_train_T1 = pd.merge(df_rad_features_T1_train, df_labels_train, on='PatientID', how='inner')\n",
    "data_test_T1 = pd.merge(df_rad_features_T1_test, df_labels_test, on='PatientID', how='inner')\n",
    "\n",
    "\n",
    "print(np.shape(data_train_T0))\n",
    "print(np.shape(data_test_T0))\n",
    "print(np.shape(data_train_T1))\n",
    "print(np.shape(data_test_T1))\n",
    "#print(data_train_T0.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed96e6da-2d39-4b77-a79a-ef5863fb7b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features for T0 training set =  (254, 1109)\n",
      "number of features for T1 training set =  (254, 1109)\n",
      "number of features for T0 test set =  (208, 1109)\n",
      "number of features for T1 test set =  (208, 1109)\n"
     ]
    }
   ],
   "source": [
    "# Check if the folder exists, and create it if it doesn't\n",
    "#if not os.path.exists(os.path.join(main_dir, 'results')):\n",
    "#    os.makedirs(os.path.join(main_dir, 'results'))\n",
    "#if not os.path.exists(os.path.join(main_dir, 'results/'+file_name)):\n",
    "#    os.makedirs(os.path.join(main_dir, 'results/'+file_name))\n",
    "    \n",
    "# Preprocess the data\n",
    "\n",
    "# Explicitly create a copy of the data\n",
    "data_train_T0_copy = data_train_T0.copy()\n",
    "data_train_T1_copy = data_train_T1.copy()\n",
    "data_test_T0_copy = data_test_T0.copy()\n",
    "data_test_T1_copy = data_test_T1.copy()\n",
    "\n",
    "# Encoding with map\n",
    "data_train_T0_copy['label_encoded'] = data_train_T0_copy['Label'].map({'benign': 0, 'malignant': 1})\n",
    "data_test_T0_copy['label_encoded'] = data_test_T0_copy['Label'].map({'benign': 0, 'malignant': 1})\n",
    "\n",
    "# Set the target variable\n",
    "target_train= data_train_T0_copy['label_encoded'].copy()\n",
    "target_test= data_test_T0_copy['label_encoded'].copy()\n",
    "\n",
    "# Set the features as the radiomic features\n",
    "features_train_T0 = data_train_T0_copy.drop(columns=['label_encoded','Label','PatientID'])\n",
    "features_train_T1 = data_train_T1_copy.drop(columns=['Label','PatientID'])\n",
    "features_test_T0 = data_test_T0_copy.drop(columns=['label_encoded','Label','PatientID'])\n",
    "features_test_T1 = data_test_T1_copy.drop(columns=['Label','PatientID'])\n",
    "\n",
    "print(\"number of features for T0 training set = \", np.shape(features_train_T0))\n",
    "print(\"number of features for T1 training set = \", np.shape(features_train_T1))\n",
    "#print(features_train_T0.columns)\n",
    "#print(features_train_T1.columns)\n",
    "\n",
    "print(\"number of features for T0 test set = \", np.shape(features_test_T0))\n",
    "print(\"number of features for T1 test set = \", np.shape(features_test_T1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d53d4c03-8c5a-4a1e-be56-7b171cf1f0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-constant features in training data =  (254, 1109)\n",
      "number of non-constant features in test data =  (208, 1109)\n",
      "Initial number of features =  1109\n",
      "(254, 1109)\n",
      "(208, 1109)\n",
      "(254,)\n",
      "(208,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming features_train_T0, features_train_T1, features_test_T0, and features_test_T1 \n",
    "# are pandas DataFrames and have the same structure and corresponding indices after the drop operation\n",
    "\n",
    "# Calculate the delta for the training features\n",
    "features_train = features_train_T0 - features_train_T1\n",
    "\n",
    "\n",
    "# Calculate the delta for the test features\n",
    "features_test = features_test_T0 - features_test_T1\n",
    "\n",
    "feature_names = features_train.columns.tolist()\n",
    "\n",
    "\n",
    "# Assuming features_train and features_add are your DataFrames\n",
    "# Concatenate them side by side (adding columns from features_add to features_train)\n",
    "#features_train_combined = pd.concat([features_train, features_train_T0], axis=1)\n",
    "#features_test_combined = pd.concat([features_test, features_test_T0], axis=1)\n",
    "\n",
    "#This is to have only delta \n",
    "features_train_combined = features_train\n",
    "features_test_combined = features_test\n",
    "\n",
    "\n",
    "\n",
    "#Remove constant radiomic features\n",
    "constant_features = features_train_combined.columns[features_train_combined.nunique() == 1]\n",
    "features_train_combined.drop(constant_features, axis=1, inplace=True)\n",
    "features_test_combined.drop(constant_features, axis=1, inplace=True)\n",
    "print(\"number of non-constant features in training data = \", np.shape(features_train_combined))\n",
    "print(\"number of non-constant features in test data = \", np.shape(features_test_combined))\n",
    "\n",
    "\n",
    "# Store the feature names\n",
    "feature_names = features_train_combined.columns.tolist()\n",
    "print(\"Initial number of features = \" , len(feature_names))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = features_train_combined\n",
    "X_validation =  features_test_combined\n",
    "y_train = target_train\n",
    "y_validation = target_test\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_validation))\n",
    "\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1228e1da-8b3e-465d-9cdf-b64151e52da2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 1109)\n",
      "(208, 1109)\n",
      "(208, 1109)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(features_test))\n",
    "print(np.shape(features_test_T0))\n",
    "print(np.shape(features_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc362248-911a-4e3e-ae0c-4435e156ee69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 1109)\n",
      "(208, 1109)\n",
      "(254,)\n",
      "(208,)\n"
     ]
    }
   ],
   "source": [
    "# Identify NaN rows for training and validation sets\n",
    "nan_rows_train = X_train.index[X_train.isnull().any(axis=1)]\n",
    "nan_rows_validation = X_validation.index[X_validation.isnull().any(axis=1)]\n",
    "\n",
    "# Remove NaN rows from the training set\n",
    "X_train.drop(index=nan_rows_train, inplace=True)\n",
    "y_train.drop(index=nan_rows_train, inplace=True)\n",
    "\n",
    "# Remove NaN rows from the validation set\n",
    "X_validation.drop(index=nan_rows_validation, inplace=True)\n",
    "y_validation.drop(index=nan_rows_validation, inplace=True)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_validation))\n",
    "\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddbc8a50-419a-4a95-a59e-592aa1b9e33e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive_train =  86\n",
      "number of negative_train =  168\n",
      "number of positive_test =  75\n",
      "number of negative_test =  133\n",
      "1109\n"
     ]
    }
   ],
   "source": [
    "X_train_init = X_train.copy()\n",
    "X_validation_init = X_validation.copy()\n",
    "\n",
    "# Scale the features of the balanced training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "# Separate positive and negative cases in the training set\n",
    "positive_cases_X_train = X_train[(y_train == 1)]\n",
    "negative_cases_X_train = X_train[(y_train == 0)]\n",
    "\n",
    "# Separate positive and negative cases in the test set\n",
    "positive_cases_X_test = X_validation[(y_validation == 1)]\n",
    "negative_cases_X_test = X_validation[(y_validation == 0)]\n",
    "\n",
    "\n",
    "print(\"number of positive_train = \", len(positive_cases_X_train))\n",
    "print(\"number of negative_train = \", len(negative_cases_X_train))\n",
    "\n",
    "print(\"number of positive_test = \", len(positive_cases_X_test))\n",
    "print(\"number of negative_test = \", len(negative_cases_X_test))\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32a63669-3f8c-4550-92f0-29669bcc729b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test after Lasso =  (208, 38)\n",
      "Shape of X_train after Lasso =  (254, 38)\n",
      "Shape of y_test after Lasso =  (208,)\n",
      "No. of features after Lasso =  38\n"
     ]
    }
   ],
   "source": [
    "# Assuming feature_names is defined and contains the names of your features\n",
    "# Assuming X_train_scaled, y_train, X_validation_scaled, and y_validation are already defined\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "# For classification, define an appropriate scorer. For example, using accuracy:\n",
    "def my_scorer(y_test, y_pred):\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "my_func = make_scorer(my_scorer, greater_is_better=True)\n",
    "\n",
    "# Use Logistic Regression with L1 penalty for feature selection\n",
    "lasso_logistic = LogisticRegression(penalty='l1', solver='liblinear', max_iter=5000)\n",
    "\n",
    "param_grid = {'C': [1 / (i * 0.5) for i in range(1, 100)]}  # Note: C is the inverse of alpha\n",
    "\n",
    "grid_search = GridSearchCV(lasso_logistic, param_grid, cv=cv, scoring=my_func)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_C = grid_search.best_params_['C']\n",
    "lasso_logistic_best = LogisticRegression(penalty='l1', solver='liblinear', C=best_C, max_iter=5000)\n",
    "lasso_logistic_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "coefficients = lasso_logistic_best.coef_[0]  # Get the coefficients of the fitted model\n",
    "zero_coefficient_indices = list(np.where(coefficients == 0)[0])\n",
    "\n",
    "X_train_scaled_lasso = X_train_scaled[:, ~np.isin(np.arange(X_train_scaled.shape[1]), zero_coefficient_indices)]\n",
    "X_validation_scaled_lasso = X_validation_scaled[:, ~np.isin(np.arange(X_validation_scaled.shape[1]), zero_coefficient_indices)]\n",
    "\n",
    "# Get the indices of the selected features after Lasso-based feature selection\n",
    "selected_feature_indices_after_lasso = np.where(~np.isin(np.arange(X_train_scaled.shape[1]), zero_coefficient_indices))[0]\n",
    "\n",
    "# Get the names of the features after Lasso-based feature selection\n",
    "selected_feature_names_after_lasso = [feature_names[idx] for idx in selected_feature_indices_after_lasso]\n",
    "\n",
    "final_selected_features = selected_feature_names_after_lasso \n",
    "\n",
    "\n",
    "X_train_scaled_lasso_df = pd.DataFrame(X_train_scaled_lasso, columns=final_selected_features)\n",
    "y_train_df = pd.DataFrame({'y_train': y_train})\n",
    "\n",
    "X_test_scaled_lasso_df = pd.DataFrame(X_validation_scaled_lasso, columns=final_selected_features)\n",
    "y_test_df = pd.DataFrame({'y_test': y_validation})\n",
    "\n",
    "# Concatenate the DataFrames horizontally (side by side)\n",
    "combined_df_train = pd.concat([X_train_scaled_lasso_df, y_train_df], axis=1)\n",
    "combined_df_test = pd.concat([X_test_scaled_lasso_df, y_test_df], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df_train.to_csv(os.path.join(main_dir,'data/Training-Lasso-scaled-combat_man_notdropped-Delta1.csv'), index=False, float_format='%.7f')\n",
    "combined_df_test.to_csv(os.path.join(main_dir,'data/Test-Lasso-scaled-combat_man_notdropped-Delta1.csv'), index=False, float_format='%.7f')\n",
    "\n",
    "\n",
    "print(\"Shape of X_test after Lasso = \", np.shape(X_validation_scaled_lasso))\n",
    "print(\"Shape of X_train after Lasso = \", np.shape(X_train_scaled_lasso))\n",
    "print(\"Shape of y_test after Lasso = \", np.shape(y_validation))\n",
    "print(\"No. of features after Lasso = \", len(final_selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb853af-f501-4860-9bc8-499530cd054e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
