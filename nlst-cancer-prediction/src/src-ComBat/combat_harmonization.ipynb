{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b245837e-cf6a-4b3b-9973-fdea326f1679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neuroCombat as nC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ranksums, ttest_ind, ttest_rel, ks_2samp\n",
    "import os\n",
    "import Nestedcombat as nested\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#import GMMComBat as gmmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad09b87-a265-4188-93a1-006d1b213949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1694, 1239)\n",
      "(1959, 14)\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "main_dir= \"/home/ulaval.ca/lesee/projects/Project-NLST/\"\n",
    "\n",
    "#features_cohort1_T0 = pd.read_csv(os.path.join(main_dir,'data/radiomicsfeatures_kheops-NLST-Dmitrii-Cohort2_Laptop_v2-cleaned-T2-ML.csv'))\n",
    "#batch_cohort1_T0 = pd.read_csv(os.path.join(main_dir,'data/cohort2-T2-batch.csv'))\n",
    "#clinical_cohort1_T0 = pd.read_csv(os.path.join(main_dir,'data/Cohort2_clinical.csv'))\n",
    "features = pd.read_csv(os.path.join(main_dir,'data/radiomicsfeatures_kheops-NLST-Dmitrii-Cohort1_2_Laptop_v2.csv'))\n",
    "batch = pd.read_csv(os.path.join(main_dir,'data/combat_params_cohort1_2.csv'))\n",
    "clinical = pd.read_csv(os.path.join(main_dir,'data/Cohort1_2_clinical.csv'))\n",
    "print(np.shape(features))\n",
    "print(np.shape(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74da39b3-eba4-41c1-b423-de2ee145bb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STANDARD', 'B50f', 'B30f', 'BONE', 'C', 'D', 'LUNG', 'FC51', 'FC10',\n",
      "       'FC30', 'B', 'FC02', 'A', 'B80f', 'FC01', 'B60f', 'B50s', 'B30s',\n",
      "       'FC50', 'B70f', 'FC82'],\n",
      "      dtype='object')\n",
      "Float64Index([2.5, 2.0, 3.2, 1.25, 1.0, 3.0, 1.3], dtype='float64')\n",
      "Index(['GE MEDICAL SYSTEMS', 'SIEMENS', 'Philips', 'TOSHIBA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "value_counts = batch['ConvolutionKernel'].value_counts()\n",
    "# Identify values that occur exactly once\n",
    "print(value_counts.index)\n",
    "value_counts = batch['SliceThickness'].value_counts()\n",
    "# Identify values that occur exactly once\n",
    "print(value_counts.index)\n",
    "value_counts = batch['Manufacturer'].value_counts()\n",
    "# Identify values that occur exactly once\n",
    "print(value_counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a271727-6aeb-4481-b17d-706525b51020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the custom function to categorize the kernel types\n",
    "def categorize_kernel(kernel):\n",
    "#    sharp_kernels = ['BONE', 'B80f', 'B60f', 'B50f', 'B30f', 'LUNG', 'B70f', 'B50s', 'B30s']\n",
    "#    soft_kernels = ['STANDARD', 'C', 'D', 'B', 'FC51', 'FC10', 'FC30', 'FC02', 'FC01', 'A', 'FC50', 'FC82']\n",
    "#    sharp_kernels = ['LUNG', 'D', 'B60f', 'FC30', 'B50f','FC51', 'BONE', 'B80f','B70f', 'B50s', 'B30s']\n",
    "#    soft_kernels = ['STANDARD', 'C', 'B30f', 'FC10', 'B', 'FC02', 'FC01', 'A', 'FC50', 'FC82']\n",
    "    sharp_kernels = ['D', 'B70f', 'B80f', 'BONE', 'FC82', 'LUNG', 'B60f', 'FC51', 'FC30', 'FC50']\n",
    "    soft_kernels = ['B30f', 'B30s', 'FC10', 'FC02', 'FC01', 'A',  'STANDARD', 'B', 'C', 'B50s', 'B50f']    \n",
    "    \n",
    "    if kernel in sharp_kernels:\n",
    "        return 'Sharp'\n",
    "    elif kernel in soft_kernels:\n",
    "        return 'Soft'\n",
    "    else:\n",
    "        return 'Unknown'  # For any values not explicitly listed\n",
    "    \n",
    "# Define the function for categorization\n",
    "def categorize_thickness(thickness):\n",
    "    if thickness < 2.0:\n",
    "        return 'Thin'\n",
    "    elif thickness <= 3.0:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Thick'\n",
    "\n",
    "# Apply the function to create a new column\n",
    "#batch['ThicknessCategory'] = batch['SliceThickness'].apply(categorize_thickness)\n",
    "batch['ThicknessCategory'] = batch['SliceThickness']\n",
    "\n",
    "\n",
    "# Apply the function to the original column to create a new one\n",
    "batch['KernelType'] = batch['ConvolutionKernel'].apply(categorize_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc677c03-7aab-48d0-8c6c-e39c7100828b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PatientID', 'StudyDate', 'StudyInstanceUID', 'SeriesInstanceUID',\n",
      "       'SeriesDescription', 'ImageType', 'Manufacturer', 'ModelName',\n",
      "       'ConvolutionKernel', 'ReconDiameter', 'SliceThickness', 'kVp', 'mAs',\n",
      "       'PixelSpacing', 'ThicknessCategory', 'KernelType'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(batch.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf02ffbf-5b54-4625-82fe-fb07c8a9a091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(batch_cohort1_T0.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee12741e-db6f-4cd2-b53c-a66b1917ab04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PatientID', 'StudyDate', 'SeriesDescription', 'Age', 'Smoking',\n",
      "       'De_Stag', 'Gender', 'Lesionsize', 'Race'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(clinical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6e25064-7aca-4737-910b-cf57dbe82ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'PatientID', 'StudyDate_x', 'SeriesDescription_x',\n",
      "       'Kernel', 'CtSlices', 'SegSlices', 'SeriesInstanceUID',\n",
      "       'diagnostics_Image-original_Mean', 'diagnostics_Image-original_Minimum',\n",
      "       ...\n",
      "       'Manufacturer', 'ModelName', 'ConvolutionKernel', 'ReconDiameter',\n",
      "       'SliceThickness', 'kVp', 'mAs', 'PixelSpacing', 'ThicknessCategory',\n",
      "       'KernelType'],\n",
      "      dtype='object', length=1253)\n",
      "(1694, 1253)\n"
     ]
    }
   ],
   "source": [
    "match_column = 'PatientID'\n",
    "\n",
    "# Define the columns to match on as a list\n",
    "match_columns = ['PatientID', 'SeriesInstanceUID']\n",
    "\n",
    "#Perform the joins\n",
    "# First, merge df1 and df2\n",
    "#merged_df1_df2 = pd.merge(features_cohort1_T0, batch_cohort1_T0, on=match_column, how='outer')\n",
    "merged_df1_df2 = pd.merge(features, batch, on=match_columns, how='inner')\n",
    "print(merged_df1_df2.columns)\n",
    "print(np.shape(merged_df1_df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da999d98-b320-4777-8dfd-470680d2057d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then, merge the result with df3\n",
    "#cohort1_T0_rads_batch_clinical = pd.merge(merged_df1_df2, clinical_cohort1_T0, on=match_column, how='outer')\n",
    "cohort1_2_rads_batch_clinical = pd.merge(merged_df1_df2, clinical, on=match_column, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e113c40-1b4b-4b22-b60b-a61a01864b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'PatientID', 'StudyDate_x', 'SeriesDescription_x',\n",
      "       'Kernel', 'CtSlices', 'SegSlices', 'SeriesInstanceUID',\n",
      "       'diagnostics_Image-original_Mean', 'diagnostics_Image-original_Minimum',\n",
      "       ...\n",
      "       'ThicknessCategory', 'KernelType', 'StudyDate', 'SeriesDescription',\n",
      "       'Age', 'Smoking', 'De_Stag', 'Gender', 'Lesionsize', 'Race'],\n",
      "      dtype='object', length=1261)\n",
      "(1689, 1261)\n"
     ]
    }
   ],
   "source": [
    "print(cohort1_2_rads_batch_clinical.columns)\n",
    "print(np.shape(cohort1_2_rads_batch_clinical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e087fc-4e67-43f0-b285-f90326340a80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0: 19\n",
      "1.25: 89\n",
      "2.0: 663\n",
      "2.5: 738\n",
      "3.0: 12\n",
      "3.2: 164\n"
     ]
    }
   ],
   "source": [
    "batch_list = ['ThicknessCategory']\n",
    "#define a string-based covariate datframe                       \n",
    "covars_string = pd.DataFrame()\n",
    "covars_string[categorical_cols] = filtered_data_copy[categorical_cols].copy()\n",
    "covars_string[batch_list] = filtered_data_copy[batch_list].copy()\n",
    "\n",
    "counts_batch_levels = Counter(list(covars_string[\"ThicknessCategory\"]))\n",
    "\n",
    "for element, count in sorted(counts_batch_levels.items()):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02049658-0b32-4c92-9025-f3e5e856695a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float64Index([1.3], dtype='float64')\n",
      "(1685, 1261)\n"
     ]
    }
   ],
   "source": [
    "#specify the batch list                                            \n",
    "#batch_list = ['Manufacturer', 'ModelName', 'ConvolutionKernel', 'SliceThickness']\n",
    "#batch_list = ['ThicknessCategory', 'ConvolutionKernel', 'Manufacturer']\n",
    "batch_list = ['ThicknessCategory']\n",
    "# specify the categorical and continous clinical covariates\n",
    "categorical_cols = ['Smoking', 'Gender']\n",
    "\n",
    "continuous_cols = ['Age']\n",
    "\n",
    "# Explicitly create a copy of the data to work with\n",
    "filtered_data_copy = cohort1_2_rads_batch_clinical.copy()\n",
    "\n",
    "# Initialize an empty mask (series of False values)\n",
    "# This will be used to mark rows to drop\n",
    "rows_to_drop = pd.Series(False, index=filtered_data_copy.index)\n",
    "\n",
    "# Loop over each categorical column\n",
    "for col in batch_list:\n",
    "    # Calculate the value counts for the current column\n",
    "    value_counts = filtered_data_copy[col].value_counts()\n",
    "    \n",
    "    # Identify values that occur exactly once\n",
    "    values_to_drop = value_counts[value_counts < 5].index\n",
    "    \n",
    "    # Update the mask to include rows where the current column's value is in values_to_drop\n",
    "    rows_to_drop |= filtered_data_copy[col].isin(values_to_drop)\n",
    "    print(values_to_drop)\n",
    "\n",
    "#remove batch levels with only one sample\n",
    "# Drop rows marked in the mask\n",
    "filtered_data_copy = filtered_data_copy[~rows_to_drop]\n",
    "print(np.shape(filtered_data_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3e7818b-b5ad-4986-91d4-bdf2c8ba801c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionKernel values for Toshiba:\n",
      "['FC51' 'FC02' 'FC30' 'FC10' 'FC50' 'FC01' 'FC82']\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where the Manufacturer is 'Toshiba'\n",
    "toshiba_data = filtered_data_copy[filtered_data_copy['Manufacturer'] == 'TOSHIBA']\n",
    "\n",
    "# Get unique ConvolutionKernel values used by Toshiba\n",
    "toshiba_kernels = toshiba_data['ConvolutionKernel'].unique()\n",
    "\n",
    "print(\"ConvolutionKernel values for Toshiba:\")\n",
    "print(toshiba_kernels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "654b6df3-b551-41bc-a8fc-eebc85b58bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionKernel values for GE MEDICAL SYSTEMS:\n",
      "['STANDARD' 'BONE' 'LUNG']\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where the Manufacturer is 'Toshiba'\n",
    "toshiba_data = filtered_data_copy[filtered_data_copy['Manufacturer'] == 'GE MEDICAL SYSTEMS']\n",
    "\n",
    "# Get unique ConvolutionKernel values used by Toshiba\n",
    "toshiba_kernels = toshiba_data['ConvolutionKernel'].unique()\n",
    "\n",
    "print(\"ConvolutionKernel values for GE MEDICAL SYSTEMS:\")\n",
    "print(toshiba_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92cdb4ae-a9c5-47b5-aa76-1dc294f9b9d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionKernel values for Philips:\n",
      "['C' 'D' 'A' 'B']\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where the Manufacturer is 'Toshiba'\n",
    "toshiba_data = filtered_data_copy[filtered_data_copy['Manufacturer'] == 'Philips']\n",
    "\n",
    "# Get unique ConvolutionKernel values used by Toshiba\n",
    "toshiba_kernels = toshiba_data['ConvolutionKernel'].unique()\n",
    "\n",
    "print(\"ConvolutionKernel values for Philips:\")\n",
    "print(toshiba_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "765c77f1-f2a4-4100-833a-f24cf944c0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ConvolutionKernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ConvolutionKernel'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m covars_string[categorical_cols] \u001b[38;5;241m=\u001b[39m filtered_data_copy[categorical_cols]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m covars_string[batch_list] \u001b[38;5;241m=\u001b[39m filtered_data_copy[batch_list]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 6\u001b[0m counts_batch_levels \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mlist\u001b[39m(\u001b[43mcovars_string\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConvolutionKernel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(counts_batch_levels\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ConvolutionKernel'"
     ]
    }
   ],
   "source": [
    "#define a string-based covariate datframe                       \n",
    "covars_string = pd.DataFrame()\n",
    "covars_string[categorical_cols] = filtered_data_copy[categorical_cols].copy()\n",
    "covars_string[batch_list] = filtered_data_copy[batch_list].copy()\n",
    "\n",
    "counts_batch_levels = Counter(list(covars_string[\"ConvolutionKernel\"]))\n",
    "\n",
    "for element, count in sorted(counts_batch_levels.items()):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8e44ac4-fbee-4fd9-85ad-cba2af198a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium: 1404\n",
      "Thick: 164\n",
      "Thin: 112\n"
     ]
    }
   ],
   "source": [
    "#define a string-based covariate datframe                       \n",
    "covars_string = pd.DataFrame()\n",
    "covars_string[categorical_cols] = filtered_data_copy[categorical_cols].copy()\n",
    "covars_string[batch_list] = filtered_data_copy[batch_list].copy()\n",
    "\n",
    "counts_batch_levels = Counter(list(covars_string[\"ThicknessCategory\"]))\n",
    "\n",
    "for element, count in sorted(counts_batch_levels.items()):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e488a5d-ef8d-46ba-bddb-1c7acd18c242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GE MEDICAL SYSTEMS: 827\n",
      "Philips: 168\n",
      "SIEMENS: 639\n",
      "TOSHIBA: 46\n"
     ]
    }
   ],
   "source": [
    "#define a string-based covariate datframe                       \n",
    "covars_string = pd.DataFrame()\n",
    "covars_string[categorical_cols] = filtered_data_copy[categorical_cols].copy()\n",
    "covars_string[batch_list] = filtered_data_copy[batch_list].copy()\n",
    "\n",
    "counts_batch_levels = Counter(list(covars_string[\"Manufacturer\"]))\n",
    "\n",
    "for element, count in sorted(counts_batch_levels.items()):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482266ca-e668-486f-afd0-0aa49a08d1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SliceThickness'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SliceThickness'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m median_thickness \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(\u001b[43mcovars_string\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSliceThickness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(median_thickness)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#define a string-based covariate datframe                       \u001b[39;00m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2023a/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SliceThickness'"
     ]
    }
   ],
   "source": [
    "median_thickness = np.median(covars_string['SliceThickness'])\n",
    "print(median_thickness)\n",
    "#define a string-based covariate datframe                       \n",
    "covars_string = pd.DataFrame()\n",
    "covars_string[categorical_cols] = filtered_data_copy[categorical_cols].copy()\n",
    "covars_string[batch_list] = filtered_data_copy[batch_list].copy()\n",
    "\n",
    "counts_batch_levels = Counter(list(covars_string['SliceThickness']))\n",
    "\n",
    "for element, count in sorted(counts_batch_levels.items()):\n",
    "    print(f\"{element}: {count}\")\n",
    "#    print(np.median(counts_batch_levels [0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bdc61a6-3dca-4f83-850b-f59e64376429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#drop columns not used in this work\n",
    "#filtered_data_copy.drop(columns = ['Center', 'pixel_spacing', 'StudyInstanceUID', 'Recurrence', 'OS_days', 'PFS_days', 'manufacturer'], inplace=True)\n",
    "                        \n",
    "#Remove rows with missing data\n",
    "#filtered_data_copy.dropna(inplace=True)\n",
    "#print(\"Shape of the dataset with radiomic features : \" , np.shape(filtered_data_copy))\n",
    "\n",
    "# Store the feature names\n",
    "feature_names = filtered_data_copy.columns.tolist()\n",
    "#print(feature_names)                    \n",
    "patient_name = filtered_data_copy['PatientID']     \n",
    "study_date = filtered_data_copy['StudyDate_x']\n",
    "series_uid = filtered_data_copy['SeriesInstanceUID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6349db1-9960-4e88-8cac-2559272a2ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String-based covariate columns :  Index(['Smoking', 'Gender', 'Manufacturer'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#define a string-based covariate datframe                       \n",
    "covars_string = pd.DataFrame()\n",
    "covars_string[categorical_cols] = filtered_data_copy[categorical_cols].copy()\n",
    "covars_string[batch_list] = filtered_data_copy[batch_list].copy()\n",
    "print(\"String-based covariate columns : \", covars_string.columns)\n",
    "#print(list(filtered_data_copy.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "968deaae-095a-4991-8cc1-d3a5402a7fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1109, 1689)\n",
      "(1689, 3)\n",
      "Index(['Smoking', 'Gender', 'Manufacturer'], dtype='object')\n",
      "(1689, 1)\n",
      "Index(['Age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#load the continous clinical covariate                       \n",
    "covars_quant = filtered_data_copy[continuous_cols]\n",
    "\n",
    "#specify the features\n",
    "data_df = filtered_data_copy.drop(columns=['Unnamed: 0', 'PatientID', 'StudyDate_x', 'SeriesDescription_x', 'Kernel', \n",
    "                                           'CtSlices', 'SegSlices', 'SeriesInstanceUID', 'StudyDate_y', 'StudyInstanceUID',\n",
    "                                           'SeriesDescription_y', 'ImageType', 'Manufacturer', 'ModelName', 'ConvolutionKernel', \n",
    "                                           'ReconDiameter', 'SliceThickness', 'kVp', 'mAs', 'PixelSpacing', 'StudyDate', \n",
    "                                           'SeriesDescription', 'Age', 'Smoking', 'De_Stag', 'Gender', 'Lesionsize', 'Race',\n",
    "                                           'ThicknessCategory', 'KernelType'\n",
    "                                          ])\n",
    "\n",
    "\n",
    "\n",
    "#print(data_df.columns)\n",
    "#Remove constant radiomic features\n",
    "constant_features = data_df.columns[data_df.nunique() == 1]\n",
    "data_df.drop(constant_features, axis=1, inplace=True)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "#dat = data_df.T\n",
    "dat = data_df.T.apply(pd.to_numeric)                        \n",
    "#print(dat)\n",
    "#label-encode the string covariates\n",
    "covars_cat = pd.DataFrame()\n",
    "\n",
    "\n",
    "for col in covars_string:\n",
    "    stringcol = covars_string[col]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(stringcol))\n",
    "    covars_cat[col] = le.transform(stringcol)\n",
    "#print(np.shape(data_df))\n",
    "print(np.shape(dat))\n",
    "#print(data_df.columns)\n",
    "print(np.shape(covars_cat))\n",
    "print(covars_cat.columns)\n",
    "print(np.shape(covars_quant))\n",
    "print(covars_quant.columns)\n",
    "#print(len(covars_cat[batch_list[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7732bc6b-7085-497b-8965-4adf2766f516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoking', 'Gender']\n",
      "['Age']\n",
      "Number of NaN values in covars: 0\n"
     ]
    }
   ],
   "source": [
    "covars_cat_reset = covars_cat.reset_index(drop=True)\n",
    "covars_quant_reset = covars_quant.reset_index(drop=True)\n",
    "\n",
    "#concatenate the label-enoded categorical (batch+clinical) and continous clinical covariates                         \n",
    "covars = pd.concat([covars_cat_reset, covars_quant_reset], axis=1)\n",
    "print(categorical_cols)\n",
    "print(continuous_cols)\n",
    "#print(covars)\n",
    "# Check if there are any NaN values in covars and batch_col\n",
    "covars_nan_count = np.sum(np.sum(np.isnan(data_df)))\n",
    "#batch_col_nan_count = np.sum(np.isnan(batch_col))\n",
    "\n",
    "print(\"Number of NaN values in covars:\", covars_nan_count)\n",
    "#print(\"Number of NaN values in batch_col:\", batch_col_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "913d9cbb-705f-4b5e-8dec-339f9dc477d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1109, 1689)\n",
      "ROUND 1:\n",
      "Harmonizing by Manufacturer...\n",
      "ComBat with Raw Data...\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(dat))\n",
    "#Nested Combat\n",
    "output_data = nested.NestedComBat(dat, covars, batch_list, categorical_cols=categorical_cols,\n",
    "                                   continuous_cols=continuous_cols, drop=False, write_p=False, filepath=main_dir)\n",
    "output_data_reset = output_data.reset_index(drop=True)\n",
    "patient_name_reset = patient_name.reset_index(drop=True)\n",
    "study_date_reset = study_date.reset_index(drop=True)\n",
    "series_uid_reset = series_uid.reset_index(drop=True)\n",
    "write_df = pd.concat([patient_name_reset, study_date_reset,series_uid_reset, output_data_reset], axis=1)\n",
    "write_df.to_csv(os.path.join(main_dir,'results/cohort1_2_rads_combat_man_notdropped.csv'), float_format='%.7f', index= False)\n",
    "\n",
    "\n",
    "#Combat\n",
    "#data_combat = nC.neuroCombat(dat= dat.values,\n",
    "#                          covars=covars,\n",
    "#                          batch_col=batch_list[0]\n",
    "#                          ,categorical_cols=categorical_cols,\n",
    "#                          continuous_cols=continuous_cols\n",
    "#                            )[\"data\"]\n",
    "#pd.DataFrame(data_combat).to_csv(os.path.join(main_dir,'data_combat/combat_harmonized_features_slice_new.csv'), float_format='%.7f', index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf0ab17-bda0-4559-bcf7-926e549f19d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "                       \n",
    "#nested.feature_kstest_histograms(output_data, covars, batch_list, main_dir)\n",
    "\n",
    "#f_dict = nested.MultiComBat(output_data.T, covars, batch_list, filepath=main_dir, categorical_cols=categorical_cols,\n",
    "#                          continuous_cols=continuous_cols, write_p=True, plotting=True)\n",
    "#for col in batch_list:\n",
    "#    write_df = pd.concat([patient_name, f_dict[col]])\n",
    "#    write_df.to_csv(os.path.join(main_dir, 'data_combat/combat_'+col+'_harmonized_features.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
