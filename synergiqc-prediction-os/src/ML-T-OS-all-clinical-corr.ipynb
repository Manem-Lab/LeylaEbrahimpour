{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af4c0a9-d8cd-404c-aaed-500c27a3dba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, KFold, RepeatedStratifiedKFold,  RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression\n",
    "from skrebate import ReliefF, SURF, MultiSURF\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (DotProduct, WhiteKernel, RBF, Matern, ConstantKernel, ExpSineSquared, RationalQuadratic, Product)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "#from skopt import BayesSearchCVba\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38707f7-15b7-4561-8670-f5cb64a53cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the start time\n",
    "st_initial = time.time()\n",
    "\n",
    "file_name = 'T-OS_st1_2_rad_clinical_XGB_tex_corr'\n",
    "#Load the data\n",
    "main_dir= \"/home/ulaval.ca/lesee/projects/Project2-synergiqc/OS/\"\n",
    "\n",
    "# Check if the folder exists, and create it if it doesn't\n",
    "if not os.path.exists(os.path.join(main_dir, 'results')):\n",
    "    os.makedirs(os.path.join(main_dir, 'results'))\n",
    "if not os.path.exists(os.path.join(main_dir, 'results/'+file_name)):\n",
    "    os.makedirs(os.path.join(main_dir, 'results/'+file_name))\n",
    "\n",
    "df_training_data = pd.read_csv(os.path.join(main_dir,'data/T-train_data_os_st1_2_rad_corr_tex_or.csv'))\n",
    "df_test_data = pd.read_csv(os.path.join(main_dir,'data/T-test_data_os_st1_2_rad_corr_tex_or.csv'))\n",
    "\n",
    "df_training_data_clinical = pd.read_csv(os.path.join(main_dir,'data/T-train_data_os_st1_2_clinical_corr_tex_or.csv'))\n",
    "df_test_data_clinical = pd.read_csv(os.path.join(main_dir,'data/T-test_data_os_st1_2_clinical_corr_tex_or.csv'))\n",
    "\n",
    "\n",
    "X_train_selected = df_training_data.iloc[:, :-1].values  # Select all columns except the last one\n",
    "y_train = df_training_data.iloc[:, -1].values # Select only the last column\n",
    "\n",
    "X_test_selected = df_test_data.iloc[:, :-1].values  # Select all columns except the last one\n",
    "y_test = df_test_data.iloc[:, -1].values # Select only the last column\n",
    "\n",
    "X_train_clinical_df = df_training_data_clinical  # Select all columns except the last one\n",
    "X_test_clinical_df = df_test_data_clinical  # Select all columns except the last one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623d6218-4d67-4ac5-a970-8ecc0c16a6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhyperparameter_grids = {\\n    'SVM': { 'kernel': ['linear', 'rbf'],\\n    'C': [0.01, 0.1, 1.0, 10.0, 100.0],\\n    'epsilon': [0.01, 0.1, 0.2, 0.3, 0.4]},\\n    'Ridge': { 'alpha': [0.01, 0.1, 1.0, 10.0]}\\n#    ,'RandomForest': {'n_estimators': [100, 200], 'max_depth': [None, 5, 10],  'min_samples_split': [2, 5],'min_samples_leaf': [1, 2],  'bootstrap': [True, False]},\\n#    'NeuronalNetwork': {'hidden_layer_sizes': [(100,), (150,), (300,)],'activation': ['relu', 'tanh'],'alpha': [0.0001, 0.001, 0.01],'learning_rate': ['constant','adaptive'], 'random_state': [0, 5, 10], 'solver': ['sgd']},\\n#       'GaussianProcesses' : {\\n#    'kernel': [RBF(length_scale=1.0), Matern(length_scale=1.0), RationalQuadratic(length_scale=1.0)],\\n#    'alpha': [1e-10, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 20.0],\\n#    'n_restarts_optimizer': [0, 1, 5],\\n#    'random_state': [0]},\\n#        'GradientBoosting': { 'n_estimators': [100, 200],'learning_rate': [0.01, 0.1],'max_depth': [3, 4],'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]},\\n #       'DecisionTree' : {    'max_depth': [None, 5, 10],  'min_samples_split': [2, 5, 10],  'min_samples_leaf': [1, 2, 4]}\\n}\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the hyperparameter grids for each model\n",
    "\n",
    "hyperparameter_grids = {\n",
    "    'XGBoost': {\n",
    "    'n_estimators': [100, 200, 300],           # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.1, 0.2],         # Step size at each boosting iteration\n",
    "    'max_depth': [3, 4, 5],                    # Maximum depth of each tree\n",
    "    'min_child_weight': [1, 2, 3],             # Minimum sum fof instance weight needed in a child\n",
    "    'subsample': [0.8, 0.9, 1.0],             # Fraction of samples used for fitting trees\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],      # Fraction of features used for fitting trees\n",
    "    'gamma': [0, 0.1, 0.2],                    # Minimum loss reduction required to make a further partition\n",
    "    'reg_alpha': [0, 0.1, 0.2],                # L1 regularization term on weights\n",
    "    'reg_lambda': [0, 0.1, 0.2]                # L2 regularization term on weights\n",
    "}\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "\"\"\"\n",
    "hyperparameter_grids = {\n",
    "    'SVM': { 'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.3, 0.4]},\n",
    "    'Ridge': { 'alpha': [0.01, 0.1, 1.0, 10.0]}\n",
    "#    ,'RandomForest': {'n_estimators': [100, 200], 'max_depth': [None, 5, 10],  'min_samples_split': [2, 5],'min_samples_leaf': [1, 2],  'bootstrap': [True, False]},\n",
    "#    'NeuronalNetwork': {'hidden_layer_sizes': [(100,), (150,), (300,)],'activation': ['relu', 'tanh'],'alpha': [0.0001, 0.001, 0.01],'learning_rate': ['constant','adaptive'], 'random_state': [0, 5, 10], 'solver': ['sgd']},\n",
    "#       'GaussianProcesses' : {\n",
    "#    'kernel': [RBF(length_scale=1.0), Matern(length_scale=1.0), RationalQuadratic(length_scale=1.0)],\n",
    "#    'alpha': [1e-10, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 20.0],\n",
    "#    'n_restarts_optimizer': [0, 1, 5],\n",
    "#    'random_state': [0]},\n",
    "#        'GradientBoosting': { 'n_estimators': [100, 200],'learning_rate': [0.01, 0.1],'max_depth': [3, 4],'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]},\n",
    " #       'DecisionTree' : {    'max_depth': [None, 5, 10],  'min_samples_split': [2, 5, 10],  'min_samples_leaf': [1, 2, 4]}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58314f65-50c2-465a-9549-c690eda6aa44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_features(method, X_train, y_train, X_test, n):\n",
    "    if method == 'mutual_info':\n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=n)\n",
    "    elif method == 'reliefF':\n",
    "        selector = ReliefF(n_neighbors=100, n_features_to_select=n)\n",
    "    elif method == 'surf':\n",
    "        selector = SURF(n_features_to_select=n)\n",
    "    elif method == 'multisurf':\n",
    "        selector = MultiSURF(n_features_to_select=n)\n",
    "\n",
    "    elif method == 'f_test':\n",
    "        selector = SelectKBest(score_func=f_regression, k=n)\n",
    "    else:\n",
    "        raise ValueError('Invalid feature selection method.')\n",
    "\n",
    "    selector.fit(X_train, y_train)\n",
    "#    selector.fit(X_train, y)\n",
    "    X_train_new = selector.transform(X_train)\n",
    "    X_test_new = selector.transform(X_test)\n",
    "    if method == 'mutual_info' or method == 'f_test':\n",
    "        selected_feature_indices = selector.get_support(indices=True)\n",
    "    else:\n",
    "    # Get the feature importance scores\n",
    "        feature_importances = selector.feature_importances_\n",
    "\n",
    "# Sort the features by importance scores and get the indices of the top n features\n",
    "        selected_feature_indices = np.argsort(feature_importances)[-n:]\n",
    "    return X_train_new, X_test_new,  selector\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, event_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    c_index = concordance_index(y_test, y_pred)     \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return c_index, mse, rmse, mae, r2, y_pred\n",
    "\n",
    "def my_scorer(y_test, y_predicted):\n",
    "    error = concordance_index(y_test,y_predicted)\n",
    "    return error\n",
    "\n",
    "my_func = make_scorer(my_scorer, greater_is_better=True)\n",
    "\n",
    "def X_test_after_feature_selection(X, y, method, n_features):\n",
    "    if method == 'mutual_info':\n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=n_features)\n",
    "    elif method == 'reliefF':\n",
    "        selector = ReliefF(n_neighbors=100, n_features_to_select=n_features)\n",
    "    elif method == 'surf':\n",
    "        selector = SURF(n_features_to_select=n_features)\n",
    "    elif method == 'multisurf':\n",
    "        selector = MultiSURF(n_features_to_select=n_features)\n",
    "\n",
    "    elif method == 'f_test':\n",
    "        selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "    else:\n",
    "        raise ValueError('Invalid feature selection method.')\n",
    "    selector.fit(X, y)\n",
    "#    selector.fit(X_train, y)\n",
    "    X_test_new = selector.transform(X)\n",
    "    return X_test_new\n",
    "\n",
    "def predict_with_model(X_test, best_model):\n",
    "    # Use the best_model to make predictions\n",
    "    model_prediction = best_model.predict(X_test)\n",
    "    return model_prediction\n",
    "# Define models\n",
    "\"\"\"\n",
    "models = {\n",
    "    'SVM': SVR()\n",
    "#    ,'Ridge' : Ridge(),\n",
    "#     'RandomForest': RandomForestRegressor(),\n",
    "#    'NeuronalNetwork': MLPRegressor(max_iter=100000, early_stopping=True),\n",
    "#    'GaussianProcesses': GaussianProcessRegressor( n_restarts_optimizer=10),\n",
    "#    'GradientBoosting': GradientBoostingRegressor(),\n",
    "#    'DecisionTree' : DecisionTreeRegressor()    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(tree_method=\"hist\", n_jobs = -1)   \n",
    "}\n",
    "\n",
    "# Define feature selection methods and the maximum number of features to consider\n",
    "feature_selection_methods = ['f_test'\n",
    "#                             , 'mutual_info'\n",
    "#                             , 'reliefF'\n",
    "#                             , 'surf'\n",
    "#                             , 'multisurf'\n",
    "                            ]\n",
    "                            \n",
    "# Initialize dictionaries to store results\n",
    "#models_list = ['SVM']\n",
    "#best_results = {}\n",
    "cv_scores_folds = np.array([])\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, random_state=42, n_repeats=1)  \n",
    "\n",
    "n_features_initial = 3\n",
    "n_features_final = 20\n",
    "#n_features_final=min(50,np.shape(X_train_selected)[1])\n",
    "iteration_features = list(range(n_features_initial, n_features_final + 1))\n",
    "# Initialize a list to store c-index values\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787dc24a-f902-45c7-a9e6-18213bbadd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Model =  XGBoost\n",
      "Method =  f_test\n",
      "Number of features =  3\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  4\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  5\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  6\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  7\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  8\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  9\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  10\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  11\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  12\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  13\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  14\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  15\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  16\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  17\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  18\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  19\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "Number of features =  20\n",
      "Fol No.  1\n",
      "Fol No.  2\n",
      "Fol No.  3\n",
      "            0         1         2         3         4         5         6  \\\n",
      "0   -1.127335  0.778278 -1.111670 -2.882528 -1.302664  0.418462 -0.826709   \n",
      "1    0.570786  0.310031 -0.709318 -2.312006 -0.956106 -1.222087 -2.097329   \n",
      "2   -0.254767 -1.382533  0.613452  0.789974  1.509640  0.621546  0.671862   \n",
      "3   -0.697599  1.057224 -0.765788 -0.040407 -0.804846  0.313329  0.777360   \n",
      "4    0.391895  0.255326 -0.147567  0.112570 -0.020783  0.325145  0.691036   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "694  0.944099 -0.431518  0.128007  0.522496  0.637679  0.223912  0.737408   \n",
      "695  0.207675 -0.384549 -0.102829 -0.061779  0.539310  0.661756 -1.339685   \n",
      "696  1.199645 -0.265553  0.206837  0.437504  0.482034  0.994360 -0.161740   \n",
      "697 -0.032476  0.172441 -0.290231 -0.412274  0.574928  0.339698  0.300866   \n",
      "698  0.629392 -0.787352  0.971827  1.121723  0.689384  0.932413  0.811939   \n",
      "\n",
      "            7       Age  Sex_1  Sex_2  Subtype_1.0  Subtype_2.0  Subtype_3.0  \\\n",
      "0    1.421527  0.699284      1      0            0            0            0   \n",
      "1   -1.093661  0.056465      0      1            0            0            0   \n",
      "2    1.841248 -3.157631      1      0            0            1            0   \n",
      "3    0.440109  1.470668      1      0            0            1            0   \n",
      "4    1.126164  0.442157      1      0            0            1            0   \n",
      "..        ...       ...    ...    ...          ...          ...          ...   \n",
      "694 -0.749690  1.984923      0      1            0            0            1   \n",
      "695  0.843656  0.056465      0      1            1            0            0   \n",
      "696 -0.729870  0.570721      0      1            0            1            0   \n",
      "697  0.072870 -0.200663      0      1            0            1            0   \n",
      "698  0.168072  0.827848      0      1            0            1            0   \n",
      "\n",
      "     Subtype_4.0  Subtype_5.0  Smoking_0  Smoking_1  Smoking_2  Smoking_3  \n",
      "0              1            0          0          0          1          0  \n",
      "1              0            1          0          0          1          0  \n",
      "2              0            0          0          0          1          0  \n",
      "3              0            0          0          0          1          0  \n",
      "4              0            0          0          0          1          0  \n",
      "..           ...          ...        ...        ...        ...        ...  \n",
      "694            0            0          0          0          1          0  \n",
      "695            0            0          0          0          1          0  \n",
      "696            0            0          0          0          1          0  \n",
      "697            0            0          0          0          1          0  \n",
      "698            0            0          0          0          1          0  \n",
      "\n",
      "[699 rows x 20 columns]\n",
      "Execution time for model XGBoost = 500.14899945259094 seconds\n",
      "Total Execution time = 521.8073551654816 seconds\n"
     ]
    }
   ],
   "source": [
    "# Outermost loop for models\n",
    "for model_name, model in models.items():\n",
    "    st = time.time()\n",
    "    print(\"ML Model = \", model_name)\n",
    "    param_grid = hyperparameter_grids[model_name]\n",
    "#    grid_search = GridSearchCV(model, param_grid, scoring=my_func, refit=True, cv=5, n_jobs=-1) \n",
    "    grid_search = RandomizedSearchCV(model, param_grid,scoring=my_func, refit=True, cv=5, n_jobs=-1)    \n",
    "    c_index = []\n",
    "    n_selected_features = []\n",
    "    selected_model = []\n",
    "    method_names = []\n",
    "    c_index_values_disovery =[]\n",
    "    c_index_values_validation = []\n",
    "    confidence_interval_values_discovery = []\n",
    "    confidence_interval_values_validation = []\n",
    "    # Loop for feature selection methods\n",
    "    for method in feature_selection_methods:\n",
    "        print(\"Method = \", method)\n",
    "        # Initialize a list to store cross-validation scores for each fold\n",
    "        # Initialize the best_results dictionary  \n",
    "        c_index_n_features_folds_total = []\n",
    "        best_model_n_features_folds_total = []\n",
    "        c_indices_n_features = []\n",
    "        # Create an empty list to store dataframes for each n_features\n",
    "        max_scores_dfs = []\n",
    "        # Create an empty DataFrame to store the averages\n",
    "        averages_df = pd.DataFrame(columns=['n_features', 'Grid Configuration', 'Average Mean Score-training'])\n",
    "       \n",
    "        for index, n_features in enumerate(iteration_features):\n",
    "            # Create a list to store the mean test scores for each grid configuration\n",
    "            grid_scores = []\n",
    "            grid_scores_c_index = []\n",
    "            print(\"Number of features = \", n_features)   \n",
    "            c_index_n_features = []\n",
    "            best_model_n_features = [] \n",
    "            c_indices_folds = []\n",
    "            c_indices = []\n",
    "#            fold_data = []\n",
    "                    # Loop for cross-validation folds\n",
    "            for i, (train_idx, val_idx) in enumerate(cv.split(X_train_selected)):\n",
    "                print (\"Fol No. \", i+1 )\n",
    "                X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]\n",
    "                y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "                # Loop for the number of features\n",
    "                # Store the fold data in a dictionary\n",
    "\n",
    "                # Perform feature selection on the fold using the current method           \n",
    "                X_train_fold_new, X_val_fold_new, selector = select_features(\n",
    "                        method, X_train_fold, y_train_fold, X_val_fold, n_features)\n",
    "                # Perform grid search cross-validation for hyperparameter tuning\n",
    "                grid_search.fit(X_train_fold_new, y_train_fold)\n",
    "                # Access the results for each grid configuration\n",
    "                results= pd.DataFrame(grid_search.cv_results_)\n",
    "                # Add columns for n_features and n_fold\n",
    "                results['n_features'] = n_features\n",
    "                results['n_fold'] = i+1                                \n",
    "                # Concatenate the current results with the cumulative results DataFrame             \n",
    "                results_df = pd.concat([results_df, results], ignore_index=True)\n",
    "                # Store the mean test scores for the current fold\n",
    "                grid_scores.extend(results['mean_test_score'])\n",
    "#                grid_scores_c_index.extend(results ['c_index'])\n",
    "            # Calculate the average mean_test_score and c-index for each grid configuration\n",
    "            grid_avg_scores = []\n",
    "            for grid_idx in range(len(results)):\n",
    "                grid_avg_score = np.mean(grid_scores[grid_idx::len(results)])\n",
    "                grid_avg_scores.append(grid_avg_score)\n",
    "            # Create a DataFrame for the average scores of each grid configuration\n",
    "            avg_scores_df = pd.DataFrame({\n",
    "                'n_features': [n_features] * len(grid_avg_scores),\n",
    "                'Grid Configuration': grid_search.cv_results_['params'],\n",
    "                'Average Mean Score-training': grid_avg_scores\n",
    "#                ,'Average c_index_folds': grid_avg_c_index_scores\n",
    "            })\n",
    "            # Concatenate the current averages with the cumulative averages DataFrame\n",
    "            averages_df = pd.concat([averages_df, avg_scores_df], ignore_index=True)\n",
    "\n",
    "            # Find the maximum 'Average Mean Score-training' for each feature\n",
    "            max_c_index_scores_df = avg_scores_df.groupby('n_features')['Average Mean Score-training'].max().reset_index()\n",
    "\n",
    "            # Merge max_c_index_scores_df with avg_scores_df to get the corresponding 'Grid Configuration'\n",
    "            max_scores_with_config = pd.merge(avg_scores_df, max_c_index_scores_df, on=['n_features','Average Mean Score-training'], suffixes=('', '_max'))\n",
    "            # Rename columns for clarity\n",
    "            max_scores_with_config = max_scores_with_config.rename(columns={'Grid Configuration': 'Max Grid Configuration'})           \n",
    "                \n",
    "            # Append the max_scores_with_config for this n_features to the list\n",
    "            max_scores_dfs.append(max_scores_with_config)\n",
    "\n",
    "        # Concatenate all the dataframes into a single dataframe\n",
    "        final_max_scores_df = pd.concat(max_scores_dfs, ignore_index=True)            \n",
    "        # Save the result to a CSV file\n",
    "        \n",
    "        if not os.path.exists(os.path.join(main_dir, 'results/'+file_name+'/'+model_name)):\n",
    "            os.makedirs(os.path.join(main_dir, 'results/'+file_name+'/'+model_name))        \n",
    "        \n",
    "        final_max_scores_df.to_csv(os.path.join(main_dir, 'results/'+file_name+'/'+model_name +'/'+ model_name + '_max_scores_with_config_'+ method+ '.csv'), index=False, float_format='%.7f')            \n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        results_df.to_csv(os.path.join(main_dir, 'results/'+file_name+'/'+model_name +'/'+ model_name +'_gridsearch_' + method+ '.csv'), index=False,  float_format='%.7f')\n",
    "        # Save the averages DataFrame to a CSV file\n",
    "        \n",
    "        averages_df.to_csv(os.path.join(main_dir, 'results/'+file_name+'/'+model_name +'/'+ model_name+ '_average_scores__with_config_' + method+ '.csv'), index=False,  float_format='%.7f')       \n",
    "   \n",
    "        # Find the row with the maximum average c-index within the current final_max_scores_df\n",
    "        max_row = final_max_scores_df.iloc[final_max_scores_df['Average Mean Score-training'].idxmax()]\n",
    "        # Extract the desired information from the row\n",
    "        n_features_max = max_row['n_features']\n",
    "        max_grid_config = max_row['Max Grid Configuration']\n",
    "        max_average_mean_score = max_row['Average Mean Score-training']\n",
    "        X_train_selected_final, X_test_selected_final, selector = select_features(\n",
    "            method, X_train_selected, y_train, X_test_selected, n_features_max)        \n",
    "        model_selected = model.set_params(**max_grid_config)        \n",
    "#********************************************************************\n",
    "        X_train_selected_final_df = pd.DataFrame(X_train_selected_final) \n",
    "        X_test_selected_final_df = pd.DataFrame(X_test_selected_final)\n",
    "       \n",
    "        # Standardize the \"Age\" column using the same scaler used for other continuous features\n",
    "        scaler = StandardScaler()\n",
    "#        print(X_train_clinical_df['Age'])\n",
    "        X_train_clinical_df['Age'] = scaler.fit_transform( X_train_clinical_df['Age'].values.reshape(-1, 1))\n",
    "        X_test_clinical_df['Age'] = scaler.fit_transform( X_test_clinical_df['Age'].values.reshape(-1, 1))\n",
    "        # Perform one-hot encoding for the categorical features\n",
    "        X_train_clinical_categorical_df_encoded = pd.get_dummies(X_train_clinical_df, columns=['Sex', 'Subtype', 'Smoking'])\n",
    "        X_test_clinical_categorical_df_encoded = pd.get_dummies(X_test_clinical_df, columns=['Sex', 'Subtype', 'Smoking'])               \n",
    "        \n",
    "        # Concatenate the one-hot encoded categorical features with the continuous feature (Age)\n",
    "        X_train_clinical_final_df = X_train_clinical_categorical_df_encoded\n",
    "        X_test_clinical_final_df = X_test_clinical_categorical_df_encoded     \n",
    "        \n",
    "        X_train_final = pd.concat([X_train_selected_final_df, X_train_clinical_final_df], axis=1)\n",
    "        X_test_final = pd.concat([X_test_selected_final_df, X_test_clinical_final_df], axis=1) \n",
    "        print(X_train_final)\n",
    "        X_train_final.columns = X_train_final.columns.astype(str)\n",
    "        X_test_final.columns = X_test_final.columns.astype(str)\n",
    "        # Train the model on the entire training set\n",
    "        model_selected.fit(X_train_final, y_train)\n",
    "        # Use the best_model to make predictions on the test dataset\n",
    "        validation_prediction = predict_with_model(X_test_final, model_selected)\n",
    "\n",
    "        # Calculate the average c-index using cross_val_score\n",
    "        cindex_values_discovery = cross_val_score(model_selected, X_train_final, y_train, cv=cv, scoring=my_func)\n",
    "        # Perform bootstrapping to estimate the 95% CI\n",
    "        n_bootstrap = 1000 \n",
    "\n",
    "        bootstrapped_cindex_values_discovery = []\n",
    "        bootstrapped_cindex_values_validation = []\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            num_samples = len(y_test)\n",
    "            resampled_indices_validation = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            resampled_y_test = y_test[resampled_indices_validation]\n",
    "            # Resample with replacement from the c-index values\n",
    "            resampled_cindices = np.random.choice(cindex_values_discovery, size=len(cindex_values_discovery), replace=True)\n",
    "            bootstrapped_cindex_discovery = np.mean(resampled_cindices)\n",
    "            bootstrapped_cindex_values_discovery.append(bootstrapped_cindex_discovery)\n",
    "\n",
    "            # Resample with replacement from the test dataset predictions\n",
    "            resampled_predictions = validation_prediction[resampled_indices_validation]\n",
    "        \n",
    "            # Calculate the c-index for the resampled predictions\n",
    "            bootstrapped_cindex_validation = concordance_index(resampled_y_test, resampled_predictions)\n",
    "            bootstrapped_cindex_values_validation.append(bootstrapped_cindex_validation)\n",
    "            \n",
    "        # Calculate the average of c-index for the CV on training dataset            \n",
    "        c_index_discovery = np.mean(cindex_values_discovery)\n",
    "        # Calculate the 95% confidence interval\n",
    "        confidence_interval_discovery = np.percentile(bootstrapped_cindex_values_discovery, [2.5, 97.5])          \n",
    "\n",
    "        # Calculate the c-index for the predictions on the test dataset\n",
    "        c_index_validation = concordance_index(y_test, validation_prediction)\n",
    "        # Calculate the 95% confidence interval\n",
    "        confidence_interval_validation = np.percentile(bootstrapped_cindex_values_validation, [2.5, 97.5])\n",
    "#**********************************************************\n",
    "        # Append the c-index value to the list\n",
    "        c_index_values_disovery.append(c_index_discovery)\n",
    "        c_index_values_validation.append(c_index_validation)\n",
    "        confidence_interval_values_discovery.append(confidence_interval_discovery)\n",
    "        confidence_interval_values_validation.append(confidence_interval_validation)\n",
    "        \n",
    "        method_names.append(method)\n",
    "        n_selected_features.append(n_features_max)\n",
    "# Save the trained model to a file\n",
    "# Path to the file\n",
    "        best_trained_model_path = os.path.join(main_dir,'results/'+file_name+'/'+model_name + '/'+ model_name + '_'+ method +'_n_f_'+str(n_features_max)+'_best_model.pkl')\n",
    "\n",
    "        # Save the best trained model\n",
    "        with open(best_trained_model_path, 'wb') as pickle_file:\n",
    "            joblib.dump(model_selected, pickle_file)        \n",
    "    data = {\n",
    "        'Method': method_names,\n",
    "        'n_features': n_selected_features, \n",
    "        'c_index_discovery': c_index_values_disovery,  \n",
    "        'c_index_discovery_CI95' : confidence_interval_values_discovery,\n",
    "        'c_index_validation': c_index_values_validation,\n",
    "        'c_index_validation_CI95' : confidence_interval_values_validation        \n",
    "        \n",
    "    }        \n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "    df.to_csv(os.path.join(main_dir,'results/'+file_name+'/'+model_name + '/'+ model_name + '_final_results.csv'), index=False, float_format='%.6f')               \n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time = et - st\n",
    "    print('Execution time for model '+model_name+ ' =', elapsed_time, 'seconds')\n",
    "\n",
    "# get the end time\n",
    "et_final = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et_final - st_initial\n",
    "print('Total Execution time =', elapsed_time, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
