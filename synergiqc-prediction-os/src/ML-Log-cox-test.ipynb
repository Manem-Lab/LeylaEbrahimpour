{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bb78ab-e081-429b-921b-c21a7fdb9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, KFold, RepeatedStratifiedKFold,  RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression\n",
    "from skrebate import ReliefF, SURF, MultiSURF\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (DotProduct, WhiteKernel, RBF, Matern, ConstantKernel, ExpSineSquared, RationalQuadratic, Product)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "#from skopt import BayesSearchCVba\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "from lifelines import CoxPHFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30ddc51-280c-43fe-a703-792e44e78119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lifelines.datasets import load_rossi\n",
    "\n",
    "# get the start time\n",
    "st_initial = time.time()\n",
    "\n",
    "#specify the file name so that the results for different models will be saved in subfolders of a main folder with the following name\n",
    "file_name = 'T-OS_st1_2_all_rad_clinical_corr_log_harmonized_cox'\n",
    "\n",
    "#specify the path of main folder \n",
    "main_dir= \"/home/ulaval.ca/lesee/projects/Project2-synergiqc/OS/\"\n",
    "\n",
    "# Check if the results folder and its subfolder with the \"file_name\" exists, and create it if it doesn't\n",
    "if not os.path.exists(os.path.join(main_dir, 'results')):\n",
    "    os.makedirs(os.path.join(main_dir, 'results'))\n",
    "if not os.path.exists(os.path.join(main_dir, 'results/'+file_name)):\n",
    "    os.makedirs(os.path.join(main_dir, 'results/'+file_name))\n",
    "\n",
    "#load the training and test dataset that have been created by dataset_feature_type.py code\n",
    "df_training_data = pd.read_csv(os.path.join(main_dir,'data/T-train_data_os_st1_2_rad_corr_log_harmonized.csv'))\n",
    "df_test_data = pd.read_csv(os.path.join(main_dir,'data/T-test_data_os_st1_2_rad_corr_log_harmonized.csv'))\n",
    "\n",
    "#load the clinical data for the same training and test dataset that have been created by dataset_feature_type.py code\n",
    "df_training_data_clinical = pd.read_csv(os.path.join(main_dir,'data/T-train_data_os_st1_2_clinical_corr_log_harmonized.csv'))\n",
    "df_test_data_clinical = pd.read_csv(os.path.join(main_dir,'data/T-test_data_os_st1_2_clinical_corr_log_harmonized.csv'))\n",
    "\n",
    "X_train_selected = df_training_data.iloc[:, :-1].values  # Select all columns except the last one (training features)\n",
    "y_train = df_training_data.iloc[:, -1].values # Select only the last column \n",
    "\n",
    "X_test_selected = df_test_data.iloc[:, :-1].values  # Select all columns except the last one (test features)\n",
    "y_test = df_test_data.iloc[:, -1].values # Select only the last column\n",
    "\n",
    "X_train_clinical = df_training_data_clinical.iloc[:, :-1].values  # Select all columns except the last one (training clinical data)\n",
    "X_test_clinical= df_test_data_clinical.iloc[:, :-1].values  # Select all columns except the last one (test clinical data)\n",
    "\n",
    "event_train = df_training_data_clinical.iloc[:, -1].values #Select only the last column\n",
    "event_test = df_test_data_clinical.iloc[:, -1].values #Select only the last column\n",
    "\n",
    "X_train_selected_df = pd.DataFrame(X_train_selected)  # Replace with actual feature names\n",
    "X_test_selected_df = pd.DataFrame(X_test_selected)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['OS-train'])\n",
    "event_train_df = pd.DataFrame(event_train, columns=['vital_status_train'])\n",
    "\n",
    "# Rename the columns in X_train_clinical_df and X_test_clinical_df\n",
    "clinical_columns = ['Smoking', 'Age', 'Subtype', 'Sex']\n",
    "\n",
    "# Create a DataFrame with the array and the new column names\n",
    "X_train_clinical_df = pd.DataFrame(X_train_clinical, columns=clinical_columns)\n",
    "X_test_clinical_df = pd.DataFrame(X_test_clinical, columns=clinical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d416213e-46d2-4eed-8b45-02bcd2f0d120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 4)\n",
      "(300, 4)\n",
      "     Smoking   Age  Subtype  Sex\n",
      "0        2.0  78.0      2.0  1.0\n",
      "1        2.0  75.0      4.0  1.0\n",
      "2        2.0  71.0      2.0  2.0\n",
      "3        2.0  67.0      5.0  1.0\n",
      "4        2.0  69.0      1.0  2.0\n",
      "..       ...   ...      ...  ...\n",
      "695      2.0  66.0      2.0  2.0\n",
      "696      2.0  77.0      5.0  1.0\n",
      "697      2.0  71.0      2.0  2.0\n",
      "698      2.0  66.0      2.0  1.0\n",
      "699      1.0  65.0      2.0  2.0\n",
      "\n",
      "[700 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_clinical_df))\n",
    "print(np.shape(X_test_clinical_df))\n",
    "\n",
    "print(X_train_clinical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f12d966-6f10-4554-9b86-9ae32dd62693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define the hyperparameter grids for Adaboost model\n",
    "hyperparameter_grids = {\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],  # Number of weak learners\n",
    "        'learning_rate': [0.01, 0.1, 0.2],  # Contribution of each weak learner\n",
    "        'loss': ['linear', 'square', 'exponential'],  # Loss function to use for updating weights\n",
    "        # Additional AdaBoostRegressor-specific parameters can be added here\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the hyperparameter grids for XGBoost model\n",
    "\"\"\"\n",
    "hyperparameter_grids = {\n",
    "    \n",
    "    'XGBoost': {\n",
    "    'n_estimators': [50, 100, 150],           # Reduced number of boosting rounds\n",
    "    'learning_rate': [0.05, 0.1, 0.15],       # Adjusted learning rate\n",
    "    'max_depth': [1, 3],                   # Reduced maximum depth\n",
    "    'min_child_weight': [1, 2],               # Adjusted minimum child weight\n",
    "    'subsample': [0.7, 0.9],             # Reduced subsample\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],      # Reduced colsample_bytree\n",
    "    'gamma': [0, 0.05, 0.1],                  # Adjusted gamma\n",
    "    'reg_alpha': [0, 0.05, 0.1],              # Adjusted reg_alpha\n",
    "    'reg_lambda': [0, 0.05, 0.1]              # Adjusted reg_lambda\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# Define the hyperparameter grids for each model\n",
    "hyperparameter_grids = {\n",
    "    'SVM': { 'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.3, 0.4]}\n",
    "#    ,'Ridge': { 'alpha': [0.01, 0.1, 1.0, 10.0]},\n",
    "#    'RandomForest': {'n_estimators': [100, 200], 'max_depth': [None, 5, 10],  'min_samples_split': [2, 5],'min_samples_leaf': [1, 2],  'bootstrap': [True, False]},\n",
    "#    'NeuronalNetwork': {'hidden_layer_sizes': [(100,), (150,), (300,)],'activation': ['relu', 'tanh'],'alpha': [0.0001, 0.001, 0.01],'learning_rate':['constant','adaptive'], 'random_state': [0, 5, 10], 'solver': ['sgd']},\n",
    "#   'GradientBoosting': { 'n_estimators': [100, 200],'learning_rate': [0.01, 0.1],'max_depth': [3, 4],'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]},\n",
    "#        'DecisionTree' : {    'max_depth': [None, 5, 10],  'min_samples_split': [2, 5, 10],  'min_samples_leaf': [1, 2, 4]},\n",
    "#    'AdaBoost': {\n",
    "#        'n_estimators': [50, 100, 200],  # Number of weak learners\n",
    "#        'learning_rate': [0.01, 0.1, 0.2],  # Contribution of each weak learner\n",
    "#        'loss': ['linear', 'square', 'exponential']  # Loss function to use for updating weights\n",
    "        # Additional AdaBoostRegressor-specific parameters can be added here}\n",
    "}\n",
    "\n",
    "\n",
    "#This function is not used in this code\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, event_train):\n",
    "    \"\"\"\n",
    "    Train the given model on the training data and evaluate its performance on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to train and evaluate.\n",
    "    - X_train: The feature matrix of the training data.\n",
    "    - y_train: The target values of the training data.\n",
    "    - X_test: The feature matrix of the test data.\n",
    "    - y_test: The target values of the test data.\n",
    "    - event_train: (Assumed to be an event indicator for survival analysis)\n",
    "\n",
    "    Returns:\n",
    "    - c_index: Concordance index for the predictions.\n",
    "    - mse: Mean squared error of the predictions.\n",
    "    - rmse: Root mean squared error of the predictions.\n",
    "    - mae: Mean absolute error of the predictions.\n",
    "    - r2: R-squared score of the predictions.\n",
    "    - y_pred: Predictions made by the model.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    c_index = concordance_index(y_test, y_pred)     \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return c_index, mse, rmse, mae, r2, y_pred\n",
    "\n",
    "#This function is not used in this code\n",
    "def X_test_after_feature_selection(X, y, method, n_features):\n",
    "    \"\"\"\n",
    "    Perform feature selection on the input data using the specified method and number of features.\n",
    "\n",
    "    Parameters:\n",
    "    - X: The feature matrix.\n",
    "    - y: The target values.\n",
    "    - method: The feature selection method ('mutual_info', 'reliefF', 'surf', 'multisurf', 'f_test').\n",
    "    - n_features: The number of features to select.\n",
    "\n",
    "    Returns:\n",
    "    - X_test_new: The feature matrix after feature selection.\n",
    "    \"\"\"    \n",
    "    if method == 'mutual_info':\n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=n_features)\n",
    "    elif method == 'reliefF':\n",
    "        selector = ReliefF(n_neighbors=100, n_features_to_select=n_features)\n",
    "    elif method == 'surf':\n",
    "        selector = SURF(n_features_to_select=n_features)\n",
    "    elif method == 'multisurf':\n",
    "        selector = MultiSURF(n_features_to_select=n_features)\n",
    "    elif method == 'f_test':\n",
    "        selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "    else:\n",
    "        raise ValueError('Invalid feature selection method.')\n",
    "    selector.fit(X, y)\n",
    "#    selector.fit(X_train, y)\n",
    "    X_test_new = selector.transform(X)\n",
    "    return X_test_new\n",
    "#This function is used in this code\n",
    "def select_features(method, X_train, y_train, X_test, n):\n",
    "    \"\"\"\n",
    "    Perform feature selection on the input data using the specified method and number of features.\n",
    "\n",
    "    Parameters:\n",
    "    - method: The feature selection method ('mutual_info', 'reliefF', 'surf', 'multisurf', 'f_test').\n",
    "    - X_train: The feature matrix of the training data.\n",
    "    - y_train: The target values of the training data.\n",
    "    - X_test: The feature matrix of the test data.\n",
    "    - n: The number of features to select.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_new: The feature matrix of the training data after feature selection.\n",
    "    - X_test_new: The feature matrix of the test data after feature selection.\n",
    "    - selector: The trained feature selector object.\n",
    "    \"\"\"      \n",
    "    if method == 'mutual_info':\n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=n)\n",
    "    elif method == 'reliefF':\n",
    "        selector = ReliefF(n_neighbors=100, n_features_to_select=n)\n",
    "    elif method == 'surf':\n",
    "        selector = SURF(n_features_to_select=n)\n",
    "    elif method == 'multisurf':\n",
    "        selector = MultiSURF(n_features_to_select=n)\n",
    "    elif method == 'f_test':\n",
    "        selector = SelectKBest(score_func=f_regression, k=n)\n",
    "    else:\n",
    "        raise ValueError('Invalid feature selection method.')\n",
    "\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_new = selector.transform(X_train)\n",
    "    X_test_new = selector.transform(X_test)\n",
    "#    if method == 'mutual_info' or method == 'f_test':\n",
    "#        selected_feature_indices = selector.get_support(indices=True)\n",
    "#    else:\n",
    "    # Get the feature importance scores\n",
    "#        feature_importances = selector.feature_importances_\n",
    "\n",
    "# Sort the features by importance scores and get the indices of the top n features\n",
    "#        selected_feature_indices = np.argsort(feature_importances)[-n:]\n",
    "    return X_train_new, X_test_new,  selector\n",
    "\n",
    "#This function is used in this code\n",
    "def my_scorer(y_test, y_predicted):\n",
    "    \"\"\"\n",
    "    Custom scoring function for model evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: The true target values.\n",
    "    - y_predicted: The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - error: Concordance index for the predictions.\n",
    "    \"\"\"\n",
    "    error = concordance_index(y_test,y_predicted)\n",
    "    return error\n",
    "\n",
    "#This function is used in this code\n",
    "def predict_with_model(X_test, best_model):\n",
    "    \"\"\"\n",
    "    Use the best_model to make predictions on the given feature matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - X_test: The feature matrix for making predictions.\n",
    "    - best_model: The trained machine learning model.\n",
    "\n",
    "    Returns:\n",
    "    - model_prediction: Predictions made by the model.\n",
    "    \"\"\"    \n",
    "    # Use the best_model to make predictions\n",
    "    model_prediction = best_model.predict(X_test)\n",
    "    return model_prediction\n",
    "\n",
    "\n",
    "#This function is used in this code\n",
    "\n",
    "def cox_select_features(X_train, y_train, X_test, n, event_train):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Cox Proportional Hazard model based on p-values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: The feature matrix of the training data.\n",
    "    - y_train: Overall Survival (time to event or censoring) for training data.\n",
    "    - X_test: The feature matrix of the test data.\n",
    "    - n: The number of features to select.\n",
    "    - event_train: The binary column indicating whether the event (death) has occurred for training data.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - X_train_new: The feature matrix of the training data after feature selection.\n",
    "    - X_test_new: The feature matrix of the test data after feature selection.\n",
    "    - cox_model: Fitted Cox Proportional Hazard model.\n",
    "    \"\"\"\n",
    "    cox_model = CoxPHFitter()\n",
    "    cox_model.fit(X_train, duration_col=y_train, event_col=event_train)\n",
    "\n",
    "    # Get the summary of the fitted model, including p-values\n",
    "    cox_summary = cox_model.summary\n",
    "\n",
    "    # Sort features based on p-values\n",
    "    sorted_features = cox_summary.sort_values(by='p', ascending=True)\n",
    "\n",
    "    # Select the top n features\n",
    "    selected_feature_indices = sorted_features.head(n).index.tolist()\n",
    "\n",
    "    # Get the new feature matrices\n",
    "    X_train_new = X_train[selected_feature_indices]\n",
    "    X_test_new = X_test[selected_feature_indices]\n",
    "\n",
    "    return X_train_new, X_test_new, cox_model\n",
    "\n",
    "# Create a custom scoring function (my_func) using make_scorer,\n",
    "# based on the my_scorer function, with greater_is_better set to True.\n",
    "\n",
    "my_func = make_scorer(my_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac03c5b-adda-432e-8a45-6789e7998227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Model =  SVM\n",
      "Number of features =  3\n",
      "Fol No.  1\n",
      "P-values for the top 3 features:\n",
      "covariate\n",
      "65    0.028008\n",
      "13    0.037009\n",
      "8     0.109190\n",
      "Name: p, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fol No.  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values for the top 3 features:\n",
      "covariate\n",
      "13    0.002558\n",
      "33    0.042893\n",
      "54    0.190875\n",
      "Name: p, dtype: float64\n",
      "Fol No.  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values for the top 3 features:\n",
      "covariate\n",
      "8     0.018266\n",
      "13    0.029428\n",
      "33    0.040040\n",
      "Name: p, dtype: float64\n",
      "Number of features =  4\n",
      "Fol No.  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values for the top 4 features:\n",
      "covariate\n",
      "65    0.028008\n",
      "13    0.037009\n",
      "8     0.109190\n",
      "33    0.173498\n",
      "Name: p, dtype: float64\n",
      "Fol No.  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values for the top 4 features:\n",
      "covariate\n",
      "13    0.002558\n",
      "33    0.042893\n",
      "54    0.190875\n",
      "8     0.190964\n",
      "Name: p, dtype: float64\n",
      "Fol No.  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values for the top 4 features:\n",
      "covariate\n",
      "8     0.018266\n",
      "13    0.029428\n",
      "33    0.040040\n",
      "65    0.044116\n",
      "Name: p, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulaval.ca/lesee/.local/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1266: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  self.params_ = pd.Series(params_, index=pd.Index(X.columns, name=\"covariate\"), name=\"coef\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Age  Smoking_0.0  Smoking_1.0  Smoking_2.0  Smoking_3.0  \\\n",
      "0    1.590831            0            0            1            0   \n",
      "1    1.205509            0            0            1            0   \n",
      "2    0.691745            0            0            1            0   \n",
      "3    0.177982            0            0            1            0   \n",
      "4    0.434864            0            0            1            0   \n",
      "..        ...          ...          ...          ...          ...   \n",
      "695  0.049541            0            0            1            0   \n",
      "696  1.462390            0            0            1            0   \n",
      "697  0.691745            0            0            1            0   \n",
      "698  0.049541            0            0            1            0   \n",
      "699 -0.078899            0            1            0            0   \n",
      "\n",
      "     Subtype_1.0  Subtype_2.0  Subtype_3.0  Subtype_4.0  Subtype_5.0  Sex_1.0  \\\n",
      "0              0            1            0            0            0        1   \n",
      "1              0            0            0            1            0        1   \n",
      "2              0            1            0            0            0        0   \n",
      "3              0            0            0            0            1        1   \n",
      "4              1            0            0            0            0        0   \n",
      "..           ...          ...          ...          ...          ...      ...   \n",
      "695            0            1            0            0            0        0   \n",
      "696            0            0            0            0            1        1   \n",
      "697            0            1            0            0            0        0   \n",
      "698            0            1            0            0            0        1   \n",
      "699            0            1            0            0            0        0   \n",
      "\n",
      "     Sex_2.0  \n",
      "0          0  \n",
      "1          0  \n",
      "2          1  \n",
      "3          0  \n",
      "4          1  \n",
      "..       ...  \n",
      "695        1  \n",
      "696        0  \n",
      "697        1  \n",
      "698        0  \n",
      "699        1  \n",
      "\n",
      "[700 rows x 12 columns]\n",
      "Execution time for model SVM = 30.44598126411438 seconds\n",
      "Total Execution time = 1102.919298171997 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define ML models\n",
    "\n",
    "models = {\n",
    "    'SVM': SVR()\n",
    "#    ,'Ridge' : Ridge(),\n",
    "#    'RandomForest': RandomForestRegressor(),\n",
    "#    'NeuronalNetwork': MLPRegressor(max_iter=100000, early_stopping=True),\n",
    "#    'GradientBoosting': GradientBoostingRegressor(),\n",
    "#    'DecisionTree' : DecisionTreeRegressor()\n",
    "#    'XGBoost': XGBRegressor(tree_method=\"hist\", n_jobs = -1)\n",
    "#    ,'AdaBoost': AdaBoostRegressor()\n",
    "}\n",
    "\n",
    "                            \n",
    "# Define the cross-validation strategy\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, random_state=42, n_repeats=1) \n",
    "\n",
    "#define the minimum number of features selected by FS methods\n",
    "n_features_initial = 3\n",
    "\n",
    "#define the maximum number of features selected by FS methods\n",
    "#n_features_final=min(50,np.shape(X_train_selected)[1])\n",
    "n_features_final = 4\n",
    "\n",
    "#list of umber of features selected by FS methods\n",
    "iteration_features = list(range(n_features_initial, n_features_final + 1))\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Outermost loop for models\n",
    "for model_name, model in models.items():\n",
    "    st = time.time()\n",
    "    print(\"ML Model = \", model_name)\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    param_grid = hyperparameter_grids[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=my_func, refit=True, cv=5, n_jobs=-1) \n",
    "#    grid_search = RandomizedSearchCV(model, param_grid,scoring=my_func, refit=True, cv=5, n_jobs=-1) \n",
    "    \n",
    "    #initialize lists to store data\n",
    "    n_selected_features = []\n",
    "#    method_names = []\n",
    "    c_index_values_disovery =[]\n",
    "    c_index_values_validation = []\n",
    "    confidence_interval_values_discovery = []\n",
    "    confidence_interval_values_validation = []\n",
    "    \n",
    "    # Create an empty list to store maximum score dataframes for each n_features\n",
    "    max_scores_dfs = []\n",
    "    # Create an empty DataFrame to store the average scores over folds for each n_features and Grid Config\n",
    "    averages_df = pd.DataFrame(columns=['n_features', 'Grid Configuration', 'Average Mean Score-training'])\n",
    "\n",
    "    for index, n_features in enumerate(iteration_features):\n",
    "        # Create a list to store the mean test scores for each grid configuration\n",
    "        grid_scores = []\n",
    "        grid_scores_c_index = []\n",
    "        print(\"Number of features = \", n_features)   \n",
    "\n",
    "        # Loop for cross-validation folds\n",
    "        for i, (train_idx, val_idx) in enumerate(cv.split(X_train_selected)):\n",
    "            print (\"Fol No. \", i+1 )\n",
    "            \n",
    "            X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "            event_train_fold, event_val_fold = event_train[train_idx],event_train[val_idx]\n",
    "            X_train_fold_df = pd.DataFrame(X_train_fold)  # Replace with actual feature names\n",
    "            X_val_fold_df = pd.DataFrame(X_val_fold)  # Replace with actual feature names            \n",
    "            y_train_fold_df = pd.DataFrame(y_train_fold, columns=['OS-train'])\n",
    "            event_train_fold_df = pd.DataFrame(event_train_fold, columns=['vital_status_train'])\n",
    "\n",
    "            # Merge X_train_fold_df, y_train_fold_df, and event_train_fold_df\n",
    "            merged_df = pd.concat([X_train_fold_df, y_train_fold_df, event_train_fold_df], axis=1)\n",
    "\n",
    "            cox_model = CoxPHFitter(penalizer=0.1)  # Adjust the penalizer value as needed\n",
    "            cox_model.fit(merged_df, duration_col='OS-train', event_col='vital_status_train')\n",
    "\n",
    "            # Get the summary of the fitted model, including p-values\n",
    "            cox_summary = cox_model.summary\n",
    "\n",
    "            # Sort features based on p-values\n",
    "            sorted_features = cox_summary.sort_values(by='p', ascending=True)\n",
    "#            print(sorted_features)\n",
    "\n",
    "            # Select the top n features\n",
    "            selected_feature_indices = sorted_features.head(n_features).index.tolist()\n",
    "#            print(selected_feature_indices)\n",
    "\n",
    "            # Print the p-values for the top n features\n",
    "            top_n_p_values = sorted_features.head(n_features)['p']\n",
    "            print(\"P-values for the top {} features:\".format(n_features))\n",
    "            print(top_n_p_values)\n",
    "            # Get the new feature matrices\n",
    "            X_train_fold_new = X_train_fold_df[selected_feature_indices]\n",
    "#            print(np.shape((X_train_fold_new)))\n",
    "#            print(len(y_train_fold))\n",
    "            X_val_fold_new = X_val_fold_df[selected_feature_indices]\n",
    "#            selector = cox_model   \n",
    "\n",
    "            # Perform grid search cross-validation for hyperparameter tuning\n",
    "            grid_search.fit(X_train_fold_new, y_train_fold)\n",
    "            # Access the results for each grid configuration\n",
    "            results= pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "            # Add columns for n_features and n_fold\n",
    "            results['n_features'] = n_features\n",
    "            results['n_fold'] = i+1\n",
    "\n",
    "            # Concatenate the current results with the cumulative results DataFrame             \n",
    "            results_df = pd.concat([results_df, results], ignore_index=True)\n",
    "\n",
    "            # Store the mean test scores for the current fold\n",
    "            grid_scores.extend(results['mean_test_score'])\n",
    "\n",
    "        # Calculate the average mean_test_score for each grid configuration\n",
    "        grid_avg_scores = []\n",
    "        for grid_idx in range(len(results)):\n",
    "            grid_avg_score = np.mean(grid_scores[grid_idx::len(results)])\n",
    "            grid_avg_scores.append(grid_avg_score)\n",
    "\n",
    "        # Create a DataFrame for the average scores of each grid configuration\n",
    "        avg_scores_df = pd.DataFrame({\n",
    "            'n_features': [n_features] * len(grid_avg_scores),\n",
    "            'Grid Configuration': grid_search.cv_results_['params'],\n",
    "            'Average Mean Score-training': grid_avg_scores\n",
    "        })\n",
    "\n",
    "        # Concatenate the current averages with the cumulative averages DataFrame\n",
    "        averages_df = pd.concat([averages_df, avg_scores_df], ignore_index=True)\n",
    "\n",
    "        # Find the maximum 'Average Mean Score-training' for each feature\n",
    "        max_c_index_scores_df = avg_scores_df.groupby('n_features')['Average Mean Score-training'].max().reset_index()\n",
    "\n",
    "        # Merge max_c_index_scores_df with avg_scores_df to get the corresponding 'Grid Configuration'\n",
    "        max_scores_with_config = pd.merge(avg_scores_df, max_c_index_scores_df, on=['n_features','Average Mean Score-training'], suffixes=('', '_max'))\n",
    "        # Rename columns for clarity\n",
    "        max_scores_with_config = max_scores_with_config.rename(columns={'Grid Configuration': 'Max Grid Configuration'})                      \n",
    "        # Append the max_scores_with_config for this n_features to the list\n",
    "        max_scores_dfs.append(max_scores_with_config)\n",
    "\n",
    "    # Concatenate all the dataframes into a single dataframe\n",
    "    final_max_scores_df = pd.concat(max_scores_dfs, ignore_index=True)\n",
    "\n",
    "    # Check if the subfolder with the \"model_name\" exists, and create it if it doesn't\n",
    "    if not os.path.exists(os.path.join(main_dir, 'results/'+file_name+'/'+model_name)):\n",
    "        os.makedirs(os.path.join(main_dir, 'results/'+file_name+'/'+model_name))   \n",
    "\n",
    "#        # Save the grid search DataFrame to a CSV file\n",
    "#        results_df.to_csv(os.path.join(main_dir, 'results/'+file_name+'/'+model_name +'/'+ model_name +'_gridsearch_' + method+ '.csv'), index=False,  float_format='%.7f')\n",
    "\n",
    "    # Save the averages DataFrame to a CSV file        \n",
    "#        averages_df.to_csv(os.path.join(main_dir, 'results/'+file_name+'/'+model_name +'/'+ model_name+ '_average_scores__with_config_' + method+ '.csv'), index=False,  float_format='%.7f')       \n",
    "\n",
    "    # Save the maximum scores Daraframe to a CSV file in a folder with model's name             \n",
    "    final_max_scores_df.to_csv(os.path.join(main_dir, 'results/'+file_name+'/'+model_name +'/'+ model_name + '_max_scores_with_config_'+ '.csv'), index=False, float_format='%.7f')      \n",
    "\n",
    "    # Find the row with the maximum average c-index within the current final_max_scores_df\n",
    "    max_row = final_max_scores_df.iloc[final_max_scores_df['Average Mean Score-training'].idxmax()]\n",
    "\n",
    "    # Extract the desired information from the row\n",
    "    n_features_max = max_row['n_features']\n",
    "    max_grid_config = max_row['Max Grid Configuration']\n",
    "    max_average_mean_score = max_row['Average Mean Score-training']\n",
    "\n",
    "    # Perform feature selection for the corrent FS method using the bext number of features           \n",
    "#    X_train_selected_final, X_test_selected_final, selector = cox_select_features(X_train_selected, y_train, X_test_selected, n_features_max, event_train_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Merge X_train_fold_df, y_train_fold_df, and event_train_fold_df\n",
    "    merged_df_train = pd.concat([X_train_selected_df, y_train_df, event_train_df], axis=1)\n",
    "\n",
    "    cox_model_train = CoxPHFitter(penalizer=0.1)  # Adjust the penalizer value as needed\n",
    "    cox_model_train.fit(merged_df_train, duration_col='OS-train', event_col='vital_status_train')\n",
    "\n",
    "    # Get the summary of the fitted model, including p-values\n",
    "    cox_summary_train = cox_model_train.summary\n",
    "\n",
    "    # Sort features based on p-values\n",
    "    sorted_features_train = cox_summary_train.sort_values(by='p', ascending=True)\n",
    "\n",
    "    # Select the top n features\n",
    "    selected_feature_indices_train = sorted_features_train.head(n_features_max).index.tolist()\n",
    "#            print(selected_feature_indices)\n",
    "\n",
    "    # Get the new feature matrices\n",
    "    X_train_selected_final = X_train_selected_df[selected_feature_indices_train]\n",
    "#            print(np.shape((X_train_fold_new)))\n",
    "#            print(len(y_train_fold))\n",
    "    X_test_selected_final = X_test_selected_df[selected_feature_indices_train]\n",
    "    \n",
    "    #set the model with best performing hyperparameters\n",
    "    model_selected = model.set_params(**max_grid_config)        \n",
    "#**********************************************************************************\n",
    "    # make X_train and X_test as dataframes\n",
    "    X_train_selected_final_df = pd.DataFrame(X_train_selected_final) \n",
    "    X_test_selected_final_df = pd.DataFrame(X_test_selected_final)\n",
    "\n",
    "    # Standardize the \"Age\" column using the same scaler used for other continuous features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_clinical_df['Age'] = scaler.fit_transform( X_train_clinical_df['Age'].values.reshape(-1, 1))\n",
    "    X_test_clinical_df['Age'] = scaler.fit_transform( X_test_clinical_df['Age'].values.reshape(-1, 1))\n",
    "\n",
    "    # Perform one-hot encoding for the categorical features in the clinical data\n",
    "    X_train_clinical_categorical_df_encoded = pd.get_dummies(X_train_clinical_df, columns=['Smoking', 'Subtype', 'Sex'])\n",
    "    X_test_clinical_categorical_df_encoded = pd.get_dummies(X_test_clinical_df, columns=['Smoking', 'Subtype', 'Sex'])                 \n",
    "    # Concatenate the one-hot encoded categorical features with the continuous feature (Age)\n",
    "    X_train_clinical_final_df = X_train_clinical_categorical_df_encoded\n",
    "    X_test_clinical_final_df = X_test_clinical_categorical_df_encoded \n",
    "\n",
    "    print(X_train_clinical_final_df)\n",
    "\n",
    "  # Concatenate the radiomics features with clinical features   \n",
    "    X_train_final = pd.concat([X_train_selected_final_df, X_train_clinical_final_df], axis=1)\n",
    "    X_test_final = pd.concat([X_test_selected_final_df, X_test_clinical_final_df], axis=1) \n",
    "\n",
    "    X_train_final.columns = X_train_final.columns.astype(str)\n",
    "    X_test_final.columns = X_test_final.columns.astype(str)\n",
    "\n",
    "    # Train the model on the entire training set\n",
    "    model_selected.fit(X_train_final, y_train)\n",
    "\n",
    "    # Use the best_model to make predictions on the test dataset\n",
    "    validation_prediction = predict_with_model(X_test_final, model_selected)\n",
    "\n",
    "    # Calculate the average c-index using cross_val_score\n",
    "    cindex_values_discovery = cross_val_score(model_selected, X_train_final, y_train, cv=cv, scoring=my_func)\n",
    "\n",
    "    # Perform bootstrapping to estimate the 95% CI\n",
    "    n_bootstrap = 1000 \n",
    "\n",
    "    bootstrapped_cindex_values_discovery = []\n",
    "    bootstrapped_cindex_values_validation = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        num_samples = len(y_test)\n",
    "        resampled_indices_validation = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "        resampled_y_test = y_test[resampled_indices_validation]\n",
    "        # Resample with replacement from the c-index values\n",
    "        resampled_cindices = np.random.choice(cindex_values_discovery, size=len(cindex_values_discovery), replace=True)\n",
    "        bootstrapped_cindex_discovery = np.mean(resampled_cindices)\n",
    "        bootstrapped_cindex_values_discovery.append(bootstrapped_cindex_discovery)\n",
    "\n",
    "        # Resample with replacement from the test dataset predictions\n",
    "        resampled_predictions = validation_prediction[resampled_indices_validation]\n",
    "\n",
    "        # Calculate the c-index for the resampled predictions\n",
    "        bootstrapped_cindex_validation = concordance_index(resampled_y_test, resampled_predictions)\n",
    "        bootstrapped_cindex_values_validation.append(bootstrapped_cindex_validation)\n",
    "\n",
    "    # Calculate the average of c-index for the CV on training dataset            \n",
    "    c_index_discovery = np.mean(cindex_values_discovery)\n",
    "    # Calculate the 95% confidence interval\n",
    "    confidence_interval_discovery = np.percentile(bootstrapped_cindex_values_discovery, [2.5, 97.5])           \n",
    "\n",
    "    # Calculate the c-index for the predictions on the test dataset\n",
    "    c_index_validation = concordance_index(y_test, validation_prediction)\n",
    "    # Calculate the 95% confidence interval\n",
    "    confidence_interval_validation = np.percentile(bootstrapped_cindex_values_validation, [2.5, 97.5])\n",
    "#**********************************************************\n",
    "    # Append the c-index value to the list\n",
    "    c_index_values_disovery.append(c_index_discovery)\n",
    "    c_index_values_validation.append(c_index_validation)\n",
    "    confidence_interval_values_discovery.append(confidence_interval_discovery)\n",
    "    confidence_interval_values_validation.append(confidence_interval_validation)\n",
    "\n",
    "#    method_names.append(method)\n",
    "    n_selected_features.append(n_features_max)\n",
    "    # Save the trained model to a file\n",
    "    # Path to the file\n",
    "    best_trained_model_path = os.path.join(main_dir,'results/'+file_name+'/'+model_name + '/'+ model_name + '_'+ '_n_f_'+str(n_features_max)+'_best_model.pkl')\n",
    "\n",
    "    # Save the best trained model\n",
    "    with open(best_trained_model_path, 'wb') as pickle_file:\n",
    "        joblib.dump(model_selected, pickle_file)        \n",
    "    data = {\n",
    "        'n_features': n_selected_features, \n",
    "        'c_index_discovery': c_index_values_disovery,  \n",
    "        'c_index_discovery_CI95' : confidence_interval_values_discovery,\n",
    "        'c_index_validation': c_index_values_validation,\n",
    "        'c_index_validation_CI95' : confidence_interval_values_validation        \n",
    "        \n",
    "    }        \n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "    df.to_csv(os.path.join(main_dir,'results/'+file_name+'/'+model_name + '/'+ model_name + '_final_results.csv'), index=False, float_format='%.6f')               \n",
    "\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time_model = et - st\n",
    "    print('Execution time for model '+model_name+ ' =', elapsed_time_model, 'seconds')\n",
    "\n",
    "# get the end time\n",
    "et_final = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et_final - st_initial\n",
    "print('Total Execution time =', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e5cd9-d76c-4256-a2be-82b1f7485efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
